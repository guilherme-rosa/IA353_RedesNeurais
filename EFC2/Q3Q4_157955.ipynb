{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EFC2_Q3_Q4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2RtZvG5FuU6zTBAOrQGkG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XHaK3VtuDEZ0","colab_type":"text"},"source":["# **Exercício de Fixação de Conceitos 2 - Questão 3**\n","\n","### **Enunciado**:\n","- Tomando o mesmo problema de classificação de dados da base MNIST, use o *framework* Keras, tendo o TensorFlow como *backend* e realize o treinamento de uma rede neural MLP.\n","- Busque inspiração em resultados já publicados na literatura e/ou adote o procedimento de tentativa e erro para definir, da melhor forma que você puder:\n","    - O número da camadas intermediárias.\n","    - O número de neurônios por camada.\n","    - O algoritmo de ajuste de pesos.\n","    - A taxa de *Dropout* (onde for pertinente).\n","    - O número de épocas de treinamento.\n","- Procure trabalhar com a média de várias execuções (junto a cada configuração candidata) para se chegar a um índice de desempenho mais estável.\n","- Um código que pode servir de ponto de partida é fornecido a seguir, considerando 1 camada intermediária com 512 neurônios, algoritmo de treinamento ADAM, ocorrência de *dropout* numa taxa de 50%, 5 épocas de treinamento e função de perda sendo uma forma de entropia cruzada.\n","- A sua proposta deve ser capaz de superar o desempenho dessa sujestão abaixo e você deve descrever de forma objetiva o caminho trilhado até a sua configuração final de código para a rede neural MLP, assim como uma comparação de desempenho com a sugestão abaixo.\n"]},{"cell_type":"code","metadata":{"id":"-p-nXAs8EUSW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"de578aa6-03b2-45e5-b0e6-97a748d5cc50","executionInfo":{"status":"ok","timestamp":1588638167610,"user_tz":180,"elapsed":3348,"user":{"displayName":"Guilherme Rosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghq9qh4ASVJhDN7nAE9xOe5vneq8826NC9rPKlCRw=s64","userId":"04886257781986524516"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","print(tf.__version__)\n","print(keras.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.2.0-rc3\n","2.3.0-tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RiFSOOj8LYpr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"ae7b5505-c75b-4244-c9e3-f563a3d409ca","executionInfo":{"status":"ok","timestamp":1588638168163,"user_tz":180,"elapsed":3894,"user":{"displayName":"Guilherme Rosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghq9qh4ASVJhDN7nAE9xOe5vneq8826NC9rPKlCRw=s64","userId":"04886257781986524516"}}},"source":["# Download do dataset e normalização das amostras de entrada de treinamento e teste:\n","\n","mnist = keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = x_train/255.0, x_test/255.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p-9Xiwure7F_","colab_type":"text"},"source":["### **Solução do exercício:**\n","- O objetivo desta atividade é propor uma rede MLP alternativa que apresente um desempenho superior àquela do enunciado.\n","- Como discutido nas aulas de dúvida, estamos interessados em analisar o desempenho das redes MLP (com diferentes configurações de hiperparâmetros) apenas com relação aos dados de treinamento. Deste modo, não é necessário separar uma parcela das 60.000 amostras para validação dos modelos.\n","- São treinadas 5 redes MLP para cada configuração de hiperparâmetros, sendo o desempenho final tomado como a média dos desempenhos das 5 redes obtidas após o treinamento.\n","- As métricas de desempenho investigadas são as perdas (*loss*) e a acurácia.\n"]},{"cell_type":"markdown","metadata":{"id":"tCJSvxRAJp6-","colab_type":"text"},"source":["**a) Arquitetura e treinamento propostos no enunciado:**\n","- Arquitetura:\n","  - Uma camada intermediária com 512 neurônios com funções de ativação ReLU e taxa de ocorrência de *dropout* de 50%.\n","  - Uma camada de saída com 10 neurônios com função de ativação softmax (não deve ser alterada).\n","- Treinamento: \n","  - Algoritmo adaptativo Adam, 5 épocas e 32 amostras por mini-batch (default do método fit()).\n","  - Função custo (loss): sparse_categorical_crossentropy (não deve ser alterada).\n","  - Métrica auxiliar: Acurácia (não deve ser alterada).\n","- Resultado do treinamento dos 5 modelos:\n","  - Perdas: 0.0812\n","  - Acurácia: 0.9744"]},{"cell_type":"markdown","metadata":{"id":"f5XAl4teQNnY","colab_type":"text"},"source":["**b) Impacto de diferentes funções de ativação dos neurônios da camada intermediária:**\n","- O primeiro hiperparâmetro investigado é o tipo de função de ativação dos neurônios da camada intermediária.\n","- Serão considerados as seguintes funções de ativação: sigmoide, tangente hiperbólica, ReLU e LeakyReLU.\n","- A Tabela 1 apresenta o desempenho para cada uma das funções de ativação. Como pode ser observado, o desempenho da rede piorou tanto para as funções sigmoidais quanto para a LeakyReLU. \n","- Deste modo, a ReLU será mantida como função de ativação dos neurônios da rede.\n","\n","<h><center>Tabela 1: Métricas de desempenho para cada tipo de função de ativação.</center></h>\n","\n","| Função de ativação | Perdas | Acurácia |\n","|--------------|--------|----------|\n","|Sigmoide    |0.1060  |0.9680    |\n","|tanh        |0.1364  |0.9583    |\n","|ReLU        |0.0812  |0.9744    |\n","|Leaky ReLU  |0.1755  |0.9485    |\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nvFTTN0np74N","colab_type":"text"},"source":["**c) Impacto de diferentes algoritmos de otimização:**\n","- São considerados os algoritmos de otimização SGD, SGD+momentum, NAG, Adagrad, Adadelta, Adam.\n","- Como pode ser observado na Tabela 2, a rede treinada com Adam atingiu um desempenho superior que as redes treinadas com os demais algoritmos.\n","- Desta forma, o Adam será mantido para as próximas investigações.\n","\n","<h><center>Tabela 2: Métricas de desempenho em função do algoritmo de otimização.</center></h>\n","\n","| Algoritmo de otimização | Perdas | Acurácia |\n","|--------------|--------|----------|\n","|SGD    |0.2571  |0.9266    |\n","|SGD + Momentum        |0.0967  |0.9706    |\n","|NAG        |0.0961  |0.9711    |\n","|Adagrad  |0.4596  |0.8691    |\n","|Adadelta  |1.5383  |0.5737    |\n","|Adam  |0.0812  |0.9744    |"]},{"cell_type":"markdown","metadata":{"id":"lS_6BobOp_KS","colab_type":"text"},"source":["**d) Impacto do número de neurônios da camada intermediária:**\n","- Agora passamos a analisar o impacto do número de neurônios da camada intermediária.\n","- Pode-se observar na Tabela 3 que a rede com 100 neurônios apresentou um desempenho bem inferior as demais. \n","- Já as redes com mais neurônios que aquela proposta no enunciado não apresentaram melhoras significativas no desempenho.\n","- Por essa razão e por possui um menor número de parâmetros ajustáveis, continuamos com a rede neural de 512 neurônios nas próximas análises.\n","\n","<h><center>Tabela 3: Métricas de desempenho em função do número de neurônios da camada intermediária.</center></h> \n","\n","| Nº Neurônios | Perdas | Acurácia |\n","|--------------|--------|----------|\n","|100           |0.1756  |0.9472    |  \n","|300           |0.0956  |0.9701    |\n","|512           |0.0812  |0.9744    | \n","|700           |0.0775  |0.9754    | \n","|900           |0.0751  |0.9765    | "]},{"cell_type":"markdown","metadata":{"id":"RJPYL9TFqCR6","colab_type":"text"},"source":["**e) Impacto da taxa de ocorrência de dropout da camada intermediária no desempenho da rede:**\n","- O último hiperparâmetro que falta ser modificado na camada intermediária é a taxa de ocorrência de dropout.\n","- Sabemos que o dropout é uma técnica de regularização e, consequentemente, tenta evitar que a rede sofra overfitting. \n","- Contudo, como não estamos preocupados em evitar overfitting e aumentar a capacidade de generalização, reduzir a taxa de ocorrência de dropout leva ao aumento de desempenho da rede, como pode ser verificado na Tabela 4.\n","- Deste modo, para obter um modelo que supera o desempenho da rede neural proposta no enunciado, basta utilizar uma taxa de dropout inferior a 0.5.\n","\n","<h><center>Tabela 4: Métricas de desempenho em função da taxa de ocorrência de dropout na camada intermediária.</center></h>\n","\n","| Taxa de dropout | Perdas | Acurácia |\n","|-----------------|--------|----------|\n","|0.1              |0.0350  |0.9887    |\n","|0.3              |0.0531  |0.9829    |\n","|0.5              |0.0812  |0.9744    |\n","|0.7              |0.1402  |0.9575    |\n","|0.9              |0.3811  |0.8847    |"]},{"cell_type":"markdown","metadata":{"id":"3fDjgEQnu4XW","colab_type":"text"},"source":["**f) Arquitetura que supera a proposta do enunciado:**\n","- Arquitetura:\n","  - Uma camada intermediária com 512 neurônios com funções de ativação ReLU e taxa de ocorrência de *dropout* de **10%**.\n","  - Uma camada de saída com 10 neurônios com função de ativação softmax (não deve ser alterada).\n","- Treinamento: \n","  - Algoritmo adaptativo Adam, 5 épocas e 32 amostras por mini-batch (default do método fit()).\n","  - Função custo (loss): sparse_categorical_crossentropy (não deve ser alterada).\n","  - Métrica auxiliar: Acurácia (não deve ser alterada).\n","- Resultado do treinamento dos 5 modelos:\n","  - Perdas: 0.0350\n","  - Acurácia: 0.9887"]},{"cell_type":"code","metadata":{"id":"Y9Y4veWqMli4","colab_type":"code","colab":{}},"source":["num_models = 5\n","EPOCHS = 5\n","media_metricas = []\n","rates = [0.5, 0.1]\n","\n","for rate in rates:\n","    \n","    lista_metricas = { 'loss': [], 'accuracy': []}\n","    metricas = {}\n","    \n","    print(f'Rede Neural com taxa de dropout de {rate*100}%:')\n","\n","    for i in range(0, num_models):\n","\n","        model = keras.models.Sequential([\n","            keras.layers.Flatten(input_shape=x_train.shape[1:]),\n","            keras.layers.Dense(512, activation='relu'),\n","            keras.layers.Dropout(rate),\n","            keras.layers.Dense(10, activation='softmax')\n","        ])\n","\n","        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","        history = model.fit(x_train, y_train, epochs=5, verbose=False)\n","        \n","        lista_metricas['loss'].append(history.history['loss'][-1])\n","        lista_metricas['accuracy'].append(history.history['accuracy'][-1])\n","\n","    for key in lista_metricas.keys():\n","        metricas[key] = sum(lista_metricas[key])/len(lista_metricas[key])\n","\n","    media_metricas.append(metricas)\n","    print(media_metricas)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pi6SoFnJxQhE","colab_type":"text"},"source":["# **Exercício de Fixação de Conceitos 2 - Questão 4**\n","\n","### **Enunciado**:\n","- Tomando o mesmo problema de classificação de dados da base MNIST e novamente usando o *framework* Keras, tendo o TensorFlow como *backend*, realize o treinamento de uma rede neural com camadas convolucionais, usando *maxpooling* e *dropout*.\n","- Mais uma vez, é apresentada a seguir uma sugestão de código e de configuração de hiperparâmetros que pode ser tomada como ponto de partida.\n","- A sua proposta deve superar, em termos de desempenho médio, essa sugestão fornecida abaixo.\n","- Descreva de forma objetiva o caminho trilhado até sua configuração final de código.\n","- Compare os resultados (em termos de taxa de acerto de classificação) com aqueles obtidos pelos três tipos de máquinas de aprendizado adotadas nas atividades anteriores (classificador linear, ELM e MLP).\n"]},{"cell_type":"code","metadata":{"id":"mWi9sLEdJhiq","colab_type":"code","colab":{}},"source":["x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lS3j4u0Xw0Jd","colab_type":"text"},"source":["### **Parte 1 - Solução do exercício:**\n","- Diferente do que foi feito na questão 2, onde treinamos 5 redes neurais para cada configuração de hiperparâmetros, aqui são treinadas apenas 3 redes, pois o tempo de treinamento de uma rede convolucional é muito maior que de uma simples rede MLP."]},{"cell_type":"markdown","metadata":{"id":"tShSpGwww4H0","colab_type":"text"},"source":["**a) Arquitetura e treinamento propostos no enunciado:**\n","- Arquitetura:\n","  - Uma camada convolucional com 32 kernels de dimensão 3x3, stride (1,1) e com funções de ativação ReLU.\n","  - Uma segunda camada convolucional com 64 kernels de dimensão 3x3, stride (1,1) e com funções de ativação ReLU, MaxPooling de dimensão 2x2 e taxa de ocorrência de dropout de 25%.\n","  - Camada do tipo Flatten\n","  - Camada fully-connected com 128 neurônios com função de ativação ReLU e taxa de ocorrência de dropout de 50%.\n","  - Uma camada de saída com 10 neurônios com função de ativação softmax (não deve ser alterada).\n","- Treinamento: \n","  - Algoritmo adaptativo Adam, 5 épocas e 32 amostras por mini-batch (default do método fit()).\n","  - Função custo (loss): sparse_categorical_crossentropy (não deve ser alterada).\n","  - Métrica auxiliar: Acurácia (não deve ser alterada).\n","- Resultado do treinamento dos 3 modelos:\n","  - Perdas: 0.0434\n","  - Acurácia: 0.9866"]},{"cell_type":"markdown","metadata":{"id":"jNGq8RS0lwWd","colab_type":"text"},"source":["**b) Alterações na rede convolucional proposta e seus impactos no desempenho:**\n","\n","- A primeira etapa das alterações foi relacionada aos hiperparâmetros da primeira camada convolucional. Foram realizadas as seguintes modificações:\n","  - 1) Redução de 32 para 16 kernels\n","  - 2) Aumento de 32 para 64 kernels\n","  - 3) Redução do tamanho dos kernels de 3x3 para 2x2\n","  - 4) Aumento do tamanho dos kernels de 3x3 para 4x4\n","  - 5) Aumento do stride de (1, 1) para (2, 2)\n","  - 6) Inserção de uma camada de Max Pooling\n","- A Tabela 5 apresenta as métricas de desempenho, perdas e acurácia, para as redes com cada uma das alterações acima.\n","\n","<h><center>Tabela 5: Métricas de desempenho para cada uma das alterações de hiperparâmetros realizadas na primeira camada convolucional.</center></h>\n","\n","| Alteração | Perdas | Acurácia |\n","|-----------------|--------|----------|\n","|1              |0.0421  |0.9871    |\n","|2              |0.0439  |0.9864    |\n","|3              |0.0464  |0.9855    |\n","|4              |0.0409  |0.9873    |\n","|5              |0.0489  |0.9850    |\n","|6              |0.0519  |0.9841    |\n","\n","- Como pode ser observado na Tabela 5, duas alterações levaram a um aumento no desempenho: redução do número de kernels de 32 para 16 e o aumento das dimensões de cada kernel de 3x3 para 4x4.\n","- Diante disso, foram treinadas redes com essas duas alterações juntas visando alcançar um desempenho ainda maior. O resultado dessa investigação está apresentado na Tabela 6. Nota-se que o desempenho dessa configuração foi menor do que o desempenho das redes com cada uma das alterações feitas individualmente.\n","- A segunda parte das alterações foram feitas considerando a primeira camada convolucional com 32 kernels de dimensão 4x4. As modificações, cujo os desempenhos estão apresentados na Tabela 6, foram:\n","  - 7) Remoção da camada de Max Pooling da segunda camada convolucional\n","  - 8) Remoção da camada fully-connected.  \n","\n","<h><center>Tabela 6: Métricas de desempenho para a segunda parte de alterações realizadas na rede convolucional.</center></h>\n","\n","| Alteração | Perdas | Acurácia |\n","|---------------|--------|----------|\n","|1+4            |0.0430  |0.9869    |\n","|7              |0.0342  |0.9894    |\n","|8              |0.0239  |0.9924    |\n","|7+8            |0.0158  |0.9949    |\n","\n","- Pode-se observar que as modificações 7 e 8 levaram a um aumento substancial no desempenho da rede, atingindo uma acurácia de 99,49% com as duas alterações feitas conjuntamente.\n"]},{"cell_type":"markdown","metadata":{"id":"9TDwF583yLZ5","colab_type":"text"},"source":["**c) Arquitetura final que supera o desempenho da rede proposta no enunciado:**\n","- Arquitetura:\n","  - Uma camada convolucional com 32 kernels de dimensão 4x4, stride (1,1) e com funções de ativação ReLU.\n","  - Uma segunda camada convolucional com 64 kernels de dimensão 3x3, stride (1,1), com funções de ativação ReLU e taxa de ocorrência de dropout de 25%.\n","  - Camada do tipo Flatten\n","  - Uma camada de saída com 10 neurônios com função de ativação softmax (não deve ser alterada).\n","- Treinamento: \n","  - Algoritmo adaptativo Adam, 5 épocas e 32 amostras por mini-batch (default do método fit()).\n","  - Função custo (loss): sparse_categorical_crossentropy (não deve ser alterada).\n","  - Métrica auxiliar: Acurácia (não deve ser alterada).\n","- Resultado do treinamento dos 3 modelos:\n","  - Perdas: 0.0158\n","  - Acurácia: 0.9949"]},{"cell_type":"code","metadata":{"id":"ZH3pGaqDKv8f","colab_type":"code","colab":{}},"source":["# Rede proposta: 32 kernels na camada 1\n","num_models = 3\n","EPOCHS = 5\n","media_metricas = []\n","\n","lista_metricas = { 'loss': [], 'accuracy': []}\n","metricas = {}\n","\n","for i in range(0, num_models):\n","    CNN = keras.models.Sequential([\n","        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]),\n","        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n","        keras.layers.MaxPooling2D(pool_size=(2,2)),\n","        keras.layers.Dropout(0.25),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.Dropout(0.5),\n","        keras.layers.Dense(10, activation='softmax')\n","    ])\n","\n","    CNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    history = CNN.fit(x_train, y_train, epochs=5)\n","\n","    lista_metricas['loss'].append(history.history['loss'][-1])\n","    lista_metricas['accuracy'].append(history.history['accuracy'][-1])\n","\n","for key in lista_metricas.keys():\n","    metricas[key] = sum(lista_metricas[key])/len(lista_metricas[key])\n","\n","media_metricas.append(metricas)\n","print(media_metricas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ij_X7I0iI0ip","colab_type":"code","colab":{}},"source":["# Rede final:\n","num_models = 3\n","EPOCHS = 5\n","media_metricas = []\n","\n","lista_metricas = { 'loss': [], 'accuracy': []}\n","metricas = {}\n","\n","for i in range(0, num_models):\n","    CNN = keras.models.Sequential([\n","        keras.layers.Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=x_train.shape[1:]),\n","        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n","        keras.layers.Dropout(0.25),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(10, activation='softmax')\n","    ])\n","\n","    CNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    history = CNN.fit(x_train, y_train, epochs=5)\n","\n","    lista_metricas['loss'].append(history.history['loss'][-1])\n","    lista_metricas['accuracy'].append(history.history['accuracy'][-1])\n","\n","for key in lista_metricas.keys():\n","    metricas[key] = sum(lista_metricas[key])/len(lista_metricas[key])\n","\n","media_metricas.append(metricas)\n","print(media_metricas)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8CHo4314xvd","colab_type":"text"},"source":["### **Parte 2 - Comparação de todos os modelos:**\n","\n","- A Tabela 7 apresenta os desempenhos dos 4 classificadores estudados nas questões 1 a 4.\n","- Classificador Linear:\n","  - Coeficiente de Regularização: 965.8832\n","  - Critério de quadrados mínimos\n","- Máquina de Aprendizado Extremo:\n","  - Camada intermediária: 1000 neurônios com função de ativação ReLU com pesos definidos aleatoriamente de acordo com uma função de distribuição normal com média nula e desvio padrão de 0.2.\n","  - Camada de saída: 10 neurônios com função de ativação linear.\n","  - Critério de quadrados mínimos\n","- Redes MLP e convolucional: estruturas já apresentados neste *notebook* (item f da questão 3 e item c da questão 4, respectivamente).  \n","\n","<h><center>Tabela 7: Desempenho dos 4 modelos de classificadores obtidos nos exercícios dos EFCs 1 e 2 junto aos dados de treinamento.</center></h>\n","\n","| Modelo de Classificador  | Acurácia | Parâmetros ajustáveis |\n","|-------------------------------|----------|------------------|\n","|Linear                         |0.8570    |  7850            |\n","|Máquina de Aprendizado Extremo |0.9456    | 10010            |\n","|Rede MLP                       |0.9887    |407050            |\n","|Rede Convolucional             |0.9949    |357610            |\n","\n","- Como era esperado, o classificador linear apresentou o pior desempenho dentre os modelos, devido ao número reduzido de parâmetros ajustáveis e a propriedade de gerar apenas fronteiras de decisão lineares para separação das classes.\n","\n","- Na sequência está a máquina de aprendizado extremo, cujo ganho de desempenho deve-se a aplicação de funções de ativação não-lineares nos dados de entrada, tornando o modelo capaz de gerar mapeamentos (e fronteiras de decisão) não-lineares e mais flexibilidade. \n","- No entanto, o desempenho da ELM é inferior ao da rede MLP pois a flexibilidade alcançada pela ELM é menor, sendo consequência do menor número de parâmetros ajustáveis.\n","- Já as redes MLP e convolucional estudadas nesse EFC foram capazes de superar significativamente o desempenho dos modelos anteriores. No caso da MLP foi alcançado um desempenho de 98.87%, enquanto a rede convolucional atingiu 99.49%.\n","- Podemos dizer que o alto desempenho da rede MLP deve-se ao elevado nível de flexibilidade do modelo devido ao seu número elevado de parâmetros ajustáveis.\n","- Por outro lado, o desempenho alcançado pela rede convolucional deve-se às camadas convolucionais e suas propriedades, tais como:\n","  - A rede convolucional não requer a vetorização das imagens de entrada.\n","  - Leva em conta o caráter espacial das imagens.\n","  - Há uma redução significativa do número de parâmetros ajustáveis, pois as camadas convolucionais realizam compartilhamento de pesos (as duas camadas convolucionais juntas possuem apenas 19040 pesos sinápticos).\n","  - Maior capacidade de extração de atributos pelos filtros convolucionais.\n","\n","\n","\n"]}]}